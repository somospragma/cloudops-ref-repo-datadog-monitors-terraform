# ConfiguraciÃ³n de ejemplo para el mÃ³dulo de Datadog Monitors
#
# PC-IAC-026: ConfiguraciÃ³n declarativa sin valores hardcodeados
# Los valores dinÃ¡micos se inyectan en locals.tf

# ============================================================================
# Variables de Gobernanza
# ============================================================================

client      = "pragma"
project     = "ecommerce"
environment = "dev"

# ============================================================================
# ConfiguraciÃ³n de Monitors
# ============================================================================

monitors_config = {
  # Monitor de errores de Lambda
  "lambda-errors" = {
    # El nombre completo se construye en locals.tf (PC-IAC-025)
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "metric alert"
    query = "avg(last_5m):sum:aws.lambda.errors{environment:dev} by {functionname} > 5"
    
    message = <<-EOT
      {{#is_alert}}
      ğŸš¨ **ALERT**: Lambda Errors Critical
      {{/is_alert}}
      {{#is_warning}}
      âš ï¸ **WARNING**: Lambda Errors High
      {{/is_warning}}
      {{#is_recovery}}
      âœ… **RECOVERED**: Lambda Errors Normal
      {{/is_recovery}}
      
      **Function**: {{functionname.name}}
      **Error Count**: {{value}}
      **Environment**: dev
      
      @slack-aws-alarms-pragma-proyectos-internos
    EOT
    
    thresholds = {
      critical = 5
      warning  = 2
    }
    
    renotify_interval   = 60
    require_full_window = false
    priority            = 1
    on_missing_data     = "show_and_notify_no_data"
    
    additional_tags = [
      "service:lambda",
      "severity:critical",
      "metric:errors",
      "team:backend"
    ]
  }
  
  # ============================================================================
  # Monitors de RDS
  # ============================================================================
  
  # Monitor de CPU de RDS
  "rds-cpu-high" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "metric alert"
    query = "avg(last_5m):avg:aws.rds.cpuutilization{env:*} by {dbinstanceidentifier,engine,aws_account,env} > 85"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      ğŸš¨ **CRÃTICO**: CPU alta en RDS
      
      **ExplicaciÃ³n:**
      - La instancia RDS **{{dbinstanceidentifier.name}}** ({{engine.name}}) en la cuenta **{{aws_account.name}}** estÃ¡ experimentando un uso crÃ­tico de CPU con **{{value}}%**.
      - Un uso de CPU superior al 85% puede causar degradaciÃ³n severa del rendimiento, queries lentas, timeouts y conexiones rechazadas.
      
      **Detalles:**
      - INSTANCIA RDS: **{{dbinstanceidentifier.name}}**
      - ENGINE: **{{engine.name}}**
      - CUENTA AWS: **{{aws_account.name}}**
      - AMBIENTE: **{{env.name}}**
      - CPU UTILIZATION: **{{value}}%**
      - UMBRAL CRÃTICO: > 85%
      
      **Impacto:**
      - âš ï¸ Queries ejecutÃ¡ndose mÃ¡s lento de lo normal
      - âš ï¸ Posibles timeouts en aplicaciones
      - âš ï¸ Conexiones pueden ser rechazadas
      - âš ï¸ DegradaciÃ³n general del rendimiento
      - âš ï¸ Riesgo de caÃ­da del servicio si llega a 100%
      
      **ValidaciÃ³n Inmediata:**
      
      1. **Verificar mÃ©tricas de CPU en CloudWatch:**
         ```bash
         aws cloudwatch get-metric-statistics \
           --namespace AWS/RDS \
           --metric-name CPUUtilization \
           --dimensions Name=DBInstanceIdentifier,Value={{dbinstanceidentifier.name}} \
           --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
           --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
           --period 300 \
           --statistics Average,Maximum
         ```
      
      2. **Identificar queries activas (PostgreSQL):**
         ```sql
         SELECT pid, usename, application_name, client_addr, state, 
                query_start, state_change, wait_event_type, wait_event, query
         FROM pg_stat_activity
         WHERE state != 'idle'
         ORDER BY query_start;
         ```
      
      3. **Identificar queries activas (MySQL):**
         ```sql
         SHOW FULL PROCESSLIST;
         ```
      
      4. **Verificar conexiones activas:**
         ```sql
         -- PostgreSQL
         SELECT count(*) FROM pg_stat_activity;
         
         -- MySQL
         SHOW STATUS LIKE 'Threads_connected';
         ```
      
      5. **Revisar Performance Insights en AWS Console:**
         - AWS Console â†’ RDS â†’ {{dbinstanceidentifier.name}} â†’ Monitoring â†’ Performance Insights
      
      **Posibles Causas:**
      
      **Queries ProblemÃ¡ticas:**
      - ğŸŒ Queries sin optimizar (full table scans)
      - ğŸ“Š Falta de Ã­ndices apropiados
      - ğŸ”„ Queries con JOINs complejos sin optimizar
      - ğŸ“ˆ Queries con agregaciones pesadas (GROUP BY, COUNT, SUM)
      - ğŸ” Queries en loop desde la aplicaciÃ³n
      
      **Carga de Trabajo:**
      - ğŸ“ˆ Aumento inesperado de trÃ¡fico
      - ğŸ”„ Procesos batch ejecutÃ¡ndose en horario pico
      - ğŸ’¾ Operaciones de mantenimiento (VACUUM, ANALYZE)
      - ğŸ”„ ReplicaciÃ³n con lag alto
      
      **ConfiguraciÃ³n:**
      - ğŸ–¥ï¸ Instance type insuficiente para la carga
      - ğŸ”§ ParÃ¡metros de configuraciÃ³n no optimizados
      - ğŸ’¾ Falta de memoria causando mÃ¡s uso de CPU
      - ğŸ”— Demasiadas conexiones concurrentes
      
      **Acciones Recomendadas:**
      
      **Inmediato (MitigaciÃ³n):**
      1. âœ… Identificar y terminar queries problemÃ¡ticas si es seguro hacerlo
      2. âœ… Verificar si hay procesos batch que puedan pausarse
      3. âœ… Revisar si hay conexiones idle que puedan cerrarse
      4. âœ… Considerar escalar verticalmente (upgrade instance type) si es crÃ­tico
      
      **Corto Plazo (OptimizaciÃ³n):**
      1. ğŸ” Analizar queries lentas usando slow query log
      2. ğŸ“Š Identificar tablas sin Ã­ndices apropiados
      3. ğŸ”§ Optimizar queries problemÃ¡ticas
      4. ğŸ“ˆ Implementar connection pooling en aplicaciÃ³n
      5. â° Reprogramar procesos batch fuera de horario pico
      
      **Largo Plazo (PrevenciÃ³n):**
      1. ğŸ“Š Implementar monitoreo de queries lentas
      2. ğŸ” Realizar auditorÃ­as periÃ³dicas de performance
      3. ğŸ“ˆ Planificar escalamiento basado en tendencias
      4. ğŸ”§ Implementar caching (Redis, Memcached)
      5. ğŸ—ï¸ Considerar read replicas para distribuir carga de lectura
      6. ğŸ“Š Implementar particionamiento de tablas grandes
      
      **Comandos Ãštiles:**
      
      **Terminar query problemÃ¡tica (PostgreSQL):**
      ```sql
      SELECT pg_terminate_backend(pid) 
      FROM pg_stat_activity 
      WHERE pid = <PID>;
      ```
      
      **Terminar query problemÃ¡tica (MySQL):**
      ```sql
      KILL <PROCESS_ID>;
      ```
      
      **Ver queries mÃ¡s lentas (PostgreSQL con pg_stat_statements):**
      ```sql
      SELECT query, calls, total_exec_time, mean_exec_time, max_exec_time
      FROM pg_stat_statements
      ORDER BY mean_exec_time DESC
      LIMIT 10;
      ```
      {{/is_alert}}
      
      {{#is_warning}}
      @slack-aws-alarms-pragma-proyectos-internos
      
   
   
    EOT
    
    thresholds = {
      critical = 85  # 85% de uso de CPU
      warning  = 70  # 70% de uso de CPU
    }
    
    renotify_interval   = 30  # Re-notificar cada 30 minutos
    require_full_window = false
    priority            = 1
    on_missing_data     = "default"
    
    additional_tags = [
      "service:rds",
      "severity:critical",
      "metric:cpu",
      "team:database",
      "issue:performance"
    ]
  }
  
  # ============================================================================
  # Monitors de EKS - Nodos
  # ============================================================================
  
  # Monitor de memoria de nodos EKS
  "eks-nodes-memory" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "query alert"
    query = "avg(last_5m):1 - max:system.mem.pct_usable{kube_node:*,env:*} by {kube_node,eks_cluster-name,env,aws_account} > 0.85"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      **ExplicaciÃ³n:**
      - Significa que el servicio **AWS EKS** en el nodo **{{kube_node.name}}** de la cuenta **{{aws_account.name}}**, estÃ¡ presentando un aumento crÃ­tico en el consumo de **Memory** con un valor mayor al **85%**.
      
      **ValidaciÃ³n**
      - Verificar el consumo de Memory de los nodes
      - CUENTA AWS: **{{aws_account.name}}**
      - CLUSTER NAME: **{{eks_cluster-name.name}}**
      - NODE: **{{kube_node.name}}**
      - AMBIENTE: **{{env.name}}**
      {{/is_alert}}
      
      {{#is_warning}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      âš ï¸ **WARNING**: El nodo **{{kube_node.name}}** estÃ¡ alcanzando el **60%** de uso de memoria.
      
      - CUENTA AWS: **{{aws_account.name}}**
      - CLUSTER NAME: **{{eks_cluster-name.name}}**
      - NODE: **{{kube_node.name}}**
      - AMBIENTE: **{{env.name}}**
      {{/is_warning}}
      
      {{#is_recovery}}
      âœ… **RECOVERED**: El nodo **{{kube_node.name}}** ha vuelto a niveles normales de memoria.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 0.85  # 85% de uso de memoria
      warning  = 0.6   # 60% de uso de memoria
    }
    
    renotify_interval   = 0
    require_full_window = false
    priority            = 1
    on_missing_data     = "default"
    
    additional_tags = [
      "service:eks",
      "severity:critical",
      "metric:memory",
      "team:infrastructure",
      "resource:nodes"
    ]
  }
  
  # Monitor de CPU de nodos EKS
  "eks-nodes-cpu" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "query alert"
    query = "avg(last_5m):avg:system.cpu.user{kube_node:*,env:*} by {kube_node,eks_cluster-name,env,aws_account} > 85"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      **ExplicaciÃ³n:**
      - Significa que el servicio **AWS EKS** en el nodo **{{kube_node.name}}** de la cuenta **{{aws_account.name}}**, estÃ¡ presentando un aumento crÃ­tico en el consumo de **CPU** con un valor mayor al **85%**.
      
      **ValidaciÃ³n**
      - Verificar el consumo de CPU de los nodes
      - CUENTA AWS: **{{aws_account.name}}**
      - CLUSTER NAME: **{{eks_cluster-name.name}}**
      - NODE: **{{kube_node.name}}**
      - AMBIENTE: **{{env.name}}**
      {{/is_alert}}
      
      {{#is_warning}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      âš ï¸ **WARNING**: El nodo **{{kube_node.name}}** estÃ¡ alcanzando el **70%** de uso de CPU.
      
      - CUENTA AWS: **{{aws_account.name}}**
      - CLUSTER NAME: **{{eks_cluster-name.name}}**
      - NODE: **{{kube_node.name}}**
      - AMBIENTE: **{{env.name}}**
      {{/is_warning}}
      
      {{#is_recovery}}
      âœ… **RECOVERED**: El nodo **{{kube_node.name}}** ha vuelto a niveles normales de CPU.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 85  # 85% de uso de CPU
      warning  = 70  # 70% de uso de CPU
    }
    
    renotify_interval   = 0
    require_full_window = false
    priority            = 1
    on_missing_data     = "default"
    
    additional_tags = [
      "service:eks",
      "severity:critical",
      "metric:cpu",
      "team:infrastructure",
      "resource:nodes"
    ]
  }
  
  # ============================================================================
  # Monitors de EKS - Pods
  # ============================================================================
  
  # Monitor de pods en CrashLoopBackOff
  "eks-pods-crashloop" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "query alert"
    query = "max(last_5m):max:kubernetes_state.container.status_report.count.waiting{reason:crashloopbackoff,env:*} by {kube_namespace,pod_name,eks_cluster-name,env,aws_account} > 0"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      **ExplicaciÃ³n:**
      - El pod **{{pod_name.name}}** en el namespace **{{kube_namespace.name}}** estÃ¡ en estado **CrashLoopBackOff**, lo que significa que estÃ¡ fallando repetidamente al iniciar.
      
      **ValidaciÃ³n**
      - Verificar los logs del pod: `kubectl logs {{pod_name.name}} -n {{kube_namespace.name}}`
      - Verificar eventos del pod: `kubectl describe pod {{pod_name.name}} -n {{kube_namespace.name}}`
      - Verificar configuraciÃ³n del deployment
      
      **Detalles:**
      - CUENTA AWS: **{{aws_account.name}}**
      - CLUSTER NAME: **{{eks_cluster-name.name}}**
      - NAMESPACE: **{{kube_namespace.name}}**
      - POD: **{{pod_name.name}}**
      - AMBIENTE: **{{env.name}}**
      
      **Posibles Causas:**
      - Error en la aplicaciÃ³n
      - ConfiguraciÃ³n incorrecta (variables de entorno, secrets, configmaps)
      - Recursos insuficientes (CPU/Memory limits)
      - Dependencias no disponibles (bases de datos, servicios externos)
      - Imagen de contenedor incorrecta o corrupta
      {{/is_alert}}
      
      {{#is_recovery}}
      âœ… **RECOVERED**: El pod **{{pod_name.name}}** en el namespace **{{kube_namespace.name}}** ha vuelto a estado normal.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 0  # Cualquier pod en CrashLoopBackOff es crÃ­tico
    }
    
    renotify_interval   = 30  # Re-notificar cada 30 minutos si persiste
    require_full_window = false
    priority            = 1
    on_missing_data     = "default"
    
    additional_tags = [
      "service:eks",
      "severity:critical",
      "metric:pod-status",
      "team:infrastructure",
      "resource:pods",
      "issue:crashloop"
    ]
  }
  
  # ============================================================================
  # Monitors de API Gateway
  # ============================================================================
  
  # Monitor de errores 5xx en API Gateway
  "apigateway-5xx-errors" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "metric alert"
    query = "sum(last_5m):sum:aws.apigateway.5xxerror{apiname:*,env:*} by {apiname,stage,env,aws_account}.as_count() > 10"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      ğŸš¨ **CRÃTICO**: Errores 5xx en API Gateway
      
      **ExplicaciÃ³n:**
      - El API Gateway **{{apiname.name}}** en el stage **{{stage.name}}** estÃ¡ generando errores 5xx (errores del servidor), con un total de **{{value}}** errores en los Ãºltimos 5 minutos.
      - Los errores 5xx indican problemas en el backend (Lambda, integraciones) o en la configuraciÃ³n del API Gateway.
      
      **Detalles:**
      - API NAME: **{{apiname.name}}**
      - STAGE: **{{stage.name}}**
      - CUENTA AWS: **{{aws_account.name}}**
      - AMBIENTE: **{{env.name}}**
      - ERRORES 5xx: **{{value}}** en 5 minutos
      
      **Impacto:**
      - âš ï¸ Usuarios recibiendo errores del servidor
      - âš ï¸ Funcionalidad de la API comprometida
      - âš ï¸ Posible pÃ©rdida de transacciones
      - âš ï¸ Experiencia de usuario degradada
      
      **Tipos de Errores 5xx:**
      - **500 Internal Server Error**: Error en Lambda o backend
      - **502 Bad Gateway**: Lambda devolviÃ³ respuesta mal formada
      - **503 Service Unavailable**: Servicio no disponible
      - **504 Gateway Timeout**: Lambda excediÃ³ 29 segundos
      
      **ValidaciÃ³n Inmediata:**
      1. Verificar logs de API Gateway: `aws logs tail /aws/apigateway/{{apiname.name}} --follow --filter-pattern "5??"`
      2. Verificar errores de Lambda asociadas
      3. Revisar mÃ©tricas en Datadog para identificar endpoints especÃ­ficos
      4. Verificar estado de integraciones backend
      
      **Posibles Causas:**
      - **500**: Error no manejado en Lambda, problema con dependencias (DB, APIs externas)
      - **502**: Lambda devolviÃ³ formato de respuesta incorrecto, falta de headers requeridos
      - **503**: Lambda throttled, servicio backend no disponible
      - **504**: Lambda excediÃ³ 29 segundos, operaciones muy lentas
      
      **Acciones Recomendadas:**
      1. Identificar el tipo especÃ­fico de error 5xx
      2. Revisar logs de Lambda para stack traces
      3. Verificar si hay throttling en Lambda
      4. Verificar conectividad con servicios backend
      5. Implementar mejor manejo de errores
      {{/is_alert}}
      
      {{#is_warning}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      âš ï¸ **WARNING**: Errores 5xx detectados en API Gateway
      
      - API NAME: **{{apiname.name}}**
      - STAGE: **{{stage.name}}**
      - ERRORES 5xx: **{{value}}**
      
      Monitorear de cerca y revisar logs si persiste.
      {{/is_warning}}
      
      {{#is_recovery}}
      âœ… **RECOVERED**: El API Gateway **{{apiname.name}}** en el stage **{{stage.name}}** ha vuelto a niveles normales de errores.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 10  # MÃ¡s de 10 errores 5xx en 5 minutos
      warning  = 5   # MÃ¡s de 5 errores 5xx en 5 minutos
    }
    
    renotify_interval   = 30  # Re-notificar cada 30 minutos
    require_full_window = false
    priority            = 1
    on_missing_data     = "default"
    
    additional_tags = [
      "service:apigateway",
      "severity:critical",
      "metric:5xx-errors",
      "team:backend",
      "issue:server-error"
    ]
  }
  
  # Monitor de errores 4xx en API Gateway
  "apigateway-4xx-errors" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "metric alert"
    query = "sum(last_5m):sum:aws.apigateway.4xxerror{apiname:*,env:*} by {apiname,stage,env,aws_account}.as_count() > 50"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      âš ï¸ **ALERTA**: Alto nÃºmero de errores 4xx en API Gateway
      
      **ExplicaciÃ³n:**
      - El API Gateway **{{apiname.name}}** en el stage **{{stage.name}}** estÃ¡ generando errores 4xx (errores del cliente), con un total de **{{value}}** errores en los Ãºltimos 5 minutos.
      - Los errores 4xx indican problemas con las peticiones de los clientes o problemas de autenticaciÃ³n/autorizaciÃ³n.
      
      **Detalles:**
      - API NAME: **{{apiname.name}}**
      - STAGE: **{{stage.name}}**
      - CUENTA AWS: **{{aws_account.name}}**
      - AMBIENTE: **{{env.name}}**
      - ERRORES 4xx: **{{value}}** en 5 minutos
      
      **Tipos de Errores 4xx:**
      - **400 Bad Request**: PeticiÃ³n mal formada, parÃ¡metros invÃ¡lidos
      - **401 Unauthorized**: Falta de autenticaciÃ³n o token invÃ¡lido
      - **403 Forbidden**: Sin permisos para acceder al recurso
      - **404 Not Found**: Endpoint no existe
      - **429 Too Many Requests**: Rate limiting activado
      
      **ValidaciÃ³n:**
      1. Verificar logs de API Gateway: `aws logs tail /aws/apigateway/{{apiname.name}} --follow --filter-pattern "4??"`
      2. Identificar el tipo especÃ­fico de error 4xx mÃ¡s comÃºn
      3. Revisar endpoints afectados
      4. Verificar si es un patrÃ³n de ataque o error legÃ­timo
      
      **Posibles Causas:**
      - **400**: ValidaciÃ³n de input fallando, cambios en el contrato de API
      - **401**: Tokens expirados, problemas con autenticaciÃ³n
      - **403**: Problemas con autorizaciÃ³n, WAF bloqueando requests
      - **404**: Clientes usando endpoints deprecados o URLs incorrectas
      - **429**: LÃ­mites de rate limiting alcanzados, posible ataque
      
      **Acciones:**
      1. Si son 429: Verificar si es trÃ¡fico legÃ­timo o ataque
      2. Si son 401/403: Revisar configuraciÃ³n de autenticaciÃ³n
      3. Si son 400: Revisar cambios recientes en validaciones
      4. Si son 404: Comunicar endpoints correctos a clientes
      {{/is_alert}}
      
      {{#is_warning}}
      âš ï¸ **WARNING**: Incremento de errores 4xx en **{{apiname.name}}** stage **{{stage.name}}**: **{{value}}** errores.
      {{/is_warning}}
      
      {{#is_recovery}}
      âœ… **RECOVERED**: El API Gateway **{{apiname.name}}** ha vuelto a niveles normales de errores 4xx.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 50  # MÃ¡s de 50 errores 4xx en 5 minutos
      warning  = 30  # MÃ¡s de 30 errores 4xx en 5 minutos
    }
    
    renotify_interval   = 60  # Re-notificar cada 60 minutos
    require_full_window = false
    priority            = 2
    on_missing_data     = "default"
    
    additional_tags = [
      "service:apigateway",
      "severity:high",
      "metric:4xx-errors",
      "team:backend",
      "issue:client-error"
    ]
  }
  
  # Monitor de latencia alta en API Gateway
  "apigateway-high-latency" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "metric alert"
    query = "avg(last_5m):p99:aws.apigateway.latency{apiname:*,env:*} by {apiname,stage,env,aws_account} > 3000"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      ğŸŒ **CRÃTICO**: Latencia alta en API Gateway
      
      **ExplicaciÃ³n:**
      - El API Gateway **{{apiname.name}}** en el stage **{{stage.name}}** estÃ¡ experimentando latencia alta en el percentil 99 (P99) con **{{value}}ms**.
      - Esto significa que el 1% de las peticiones mÃ¡s lentas estÃ¡n tardando mÃ¡s de {{value}}ms, afectando la experiencia de usuario.
      
      **Detalles:**
      - API NAME: **{{apiname.name}}**
      - STAGE: **{{stage.name}}**
      - CUENTA AWS: **{{aws_account.name}}**
      - AMBIENTE: **{{env.name}}**
      - LATENCIA P99: **{{value}}ms**
      - UMBRAL CRÃTICO: > 3000ms (3 segundos)
      
      **Impacto:**
      - âš ï¸ Experiencia de usuario degradada
      - âš ï¸ Posibles timeouts en clientes
      - âš ï¸ Aumento de costos por ejecuciones largas
      - âš ï¸ Riesgo de timeout en API Gateway (29s)
      
      **ValidaciÃ³n:**
      1. Verificar latencia de Lambda backend:
         ```
         aws cloudwatch get-metric-statistics \
           --namespace AWS/Lambda \
           --metric-name Duration \
           --statistics Average,Maximum
         ```
      2. Revisar mÃ©tricas de latencia por endpoint en Datadog
      3. Verificar si hay cold starts frecuentes
      4. Revisar logs de X-Ray para identificar cuellos de botella
      
      **Posibles Causas:**
      - ğŸŒ Lambda con ejecuciones lentas (queries DB, APIs externas)
      - â„ï¸ Cold starts frecuentes
      - ğŸ”— Integraciones externas lentas
      - ğŸ’¾ Consultas a base de datos sin optimizar
      - ğŸŒ Latencia de red alta
      - ğŸ§  Lambda con poca memoria asignada
      - ğŸ“¦ Payload grande en request/response
      
      **Acciones Recomendadas:**
      
      **Inmediato:**
      1. Identificar endpoints especÃ­ficos con alta latencia
      2. Revisar logs de Lambda para operaciones lentas
      3. Verificar estado de servicios backend (DB, APIs)
      
      **Corto Plazo:**
      1. Optimizar queries de base de datos
      2. Implementar caching (API Gateway cache, Redis)
      3. Aumentar memoria de Lambda (mÃ¡s memoria = mÃ¡s CPU)
      4. Implementar timeouts en llamadas externas
      5. Reducir tamaÃ±o de payloads
      
      **Largo Plazo:**
      1. Implementar provisioned concurrency para evitar cold starts
      2. Refactorizar cÃ³digo ineficiente
      3. Implementar CDN para contenido estÃ¡tico
      4. Considerar arquitectura asÃ­ncrona para operaciones largas
      5. Implementar connection pooling para DB
      {{/is_alert}}
      
      {{#is_warning}}
      âš ï¸ **WARNING**: Latencia elevada en **{{apiname.name}}** stage **{{stage.name}}**: **{{value}}ms** (P99).
      {{/is_warning}}
      
      {{#is_recovery}}
      âœ… **RECOVERED**: La latencia del API Gateway **{{apiname.name}}** ha vuelto a niveles normales.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 3000  # P99 > 3 segundos
      warning  = 2000  # P99 > 2 segundos
    }
    
    renotify_interval   = 30
    require_full_window = false
    priority            = 1
    on_missing_data     = "default"
    
    additional_tags = [
      "service:apigateway",
      "severity:critical",
      "metric:latency",
      "team:backend",
      "issue:performance"
    ]
  }
  
  # Monitor de anomalÃ­as en request count de API Gateway
  "apigateway-request-anomaly" = {
    name = ""  # Se llenarÃ¡ automÃ¡ticamente en locals.tf
    
    type  = "query alert"
    query = "avg(last_4h):anomalies(sum:aws.apigateway.count{apiname:*,env:*} by {apiname,stage,env,aws_account}, 'agile', 2, direction='both', interval=60, alert_window='last_15m', count_default_zero='true') >= 1"
    
    message = <<-EOT
      {{#is_alert}}
      @slack-aws-alarms-pragma-proyectos-internos
      
      ---
      
      ğŸ” **ANOMALÃA DETECTADA**: TrÃ¡fico anormal en API Gateway
      
      **ExplicaciÃ³n:**
      - El API Gateway **{{apiname.name}}** en el stage **{{stage.name}}** estÃ¡ mostrando un patrÃ³n de trÃ¡fico anormal comparado con su comportamiento histÃ³rico.
      
      **Detalles:**
      - API NAME: **{{apiname.name}}**
      - STAGE: **{{stage.name}}**
      - CUENTA AWS: **{{aws_account.name}}**
      - AMBIENTE: **{{env.name}}**
      - PATRÃ“N: Basado en Ãºltimas 4 horas
      
      **Posibles Escenarios:**
      
      **Pico de TrÃ¡fico (Aumento Anormal):**
      - âœ… CampaÃ±a de marketing exitosa
      - âœ… Feature nuevo con alta adopciÃ³n
      - âš ï¸ Ataque DDoS o abuso
      - âš ï¸ Loop infinito en cliente
      - âš ï¸ Retry excesivo por errores
      
      **CaÃ­da de TrÃ¡fico (DisminuciÃ³n Anormal):**
      - âš ï¸ Problema en clientes o aplicaciones frontend
      - âš ï¸ Problema de DNS o conectividad
      - âš ï¸ Cambio no planificado en routing
      - âš ï¸ Servicio upstream caÃ­do
      
      **ValidaciÃ³n:**
      1. Verificar mÃ©tricas de request count en Datadog
      2. Revisar si hay errores correlacionados (5xx, 4xx)
      3. Verificar logs de acceso de API Gateway
      4. Revisar si hay cambios recientes en la aplicaciÃ³n
      5. Verificar estado de servicios upstream
      
      **Acciones:**
      1. Determinar si el cambio es esperado o anÃ³malo
      2. Si es pico: Verificar si hay throttling o errores
      3. Si es caÃ­da: Verificar conectividad y servicios upstream
      4. Revisar logs para identificar patrÃ³n
      5. Considerar ajustar rate limiting si es ataque
      {{/is_alert}}
      
      {{#is_recovery}}
      âœ… **RECUPERADO**: El trÃ¡fico del API Gateway **{{apiname.name}}** ha vuelto a patrones normales.
      {{/is_recovery}}
    EOT
    
    thresholds = {
      critical = 1  # Cualquier anomalÃ­a detectada
    }
    
    renotify_interval   = 0
    require_full_window = false
    priority            = 2
    on_missing_data     = "default"
    
    additional_tags = [
      "service:apigateway",
      "severity:medium",
      "metric:request-count",
      "team:backend",
      "issue:anomaly"
    ]
  }
}
